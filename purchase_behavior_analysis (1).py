# -*- coding: utf-8 -*-
"""Purchase Behavior Analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Xe0FXiYxs87NhAUzJXFF7Nd1chwRthTA

# CS 6220 Final Project
## Purchase Behavior Analysis for Targeted Customer Segmentation
### Authors: Jingjing Ma & Jinyan Li

## Import libraries
"""

# Ignore warnings
import warnings
warnings.filterwarnings('ignore')

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import matplotlib.gridspec as gridspec
from matplotlib import colors
import seaborn as sns
import plotly.graph_objects as go
from matplotlib.colors import LinearSegmentedColormap
from matplotlib import colors as mcolors
from scipy.stats import linregress
from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from scipy.cluster.hierarchy import dendrogram, linkage, fcluster
from sklearn.decomposition import PCA
from sklearn.neighbors import NearestNeighbors
from yellowbrick.cluster import KElbowVisualizer, SilhouetteVisualizer
from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score
from sklearn.cluster import KMeans, DBSCAN
from tabulate import tabulate
from collections import Counter
from mpl_toolkits.mplot3d import Axes3D

"""## Data inspection & cleaning"""

from google.colab import drive
drive.mount('/content/drive')

df = pd.read_csv('/content/drive/MyDrive/marketing_campaign.csv', delimiter='\t')
df.head()

df.info()

"""We will drop rows with missing values in *Income*:




"""

data = df.dropna(subset=["Income"])

"""## Feature engineering

The original datasets may not always contain variables that directly reflect the characteristics or behaviors most relevant to our segmentation goals. To enhance our analysis and achieve more meaningful segmentation, we are creating the following features based on the existing data:

* **Total_Num** - Generated from *NumWebPurchases*, *NumCatalogPurchases*, and *NumStorePurchases* by adding the total purchases from all channels for each customer. Frequency can help us estimate customer loyalty and purchasing habits since higher frequency often indicates higher customer loyalty.
* **Total_Mnt** - Generated from *MntWines, MntFruits, MntMeatProducts, MntFishProducts, MntSweetProducts, MntGoldProds*. Total amount each customer spends across all product categories. Customers with higher spending may receive targeted offers to boost profitability.
* **Age**: Generated from *Year_Birth*, indicates the customer's age.
* **Living_Situation**: Generated from *Marital_Status*, indicates whether a customer lives alone or with a partner. Customers living with partners or families might be interested in different products than those living alone.
* **Num_Children**: Combine *KidHome* and *TeenHome* to get the number of children and teenagers in the household. This can help in marketing family-oriented products and promotions.
* **Is_Parent**: Generated from *KidHome* and *TeenHome* to derive a binary indicator of whether a customer has children or not since parents may have different needs for products.
* **Family_Size**: Combine *Marital_Status* and *Num_Children* to estimate the total number of family members. Different family sizes may lead to needs for different product sizes, categories, and frequencies of purchase.
* **Education**: Simplfiy the education levels into two categories: Undergraduate and Below, Graduate and Above.
* **Campaigns_Accepted**: Generated from *AcceptedCmp1, AcceptedCmp2, AcceptedCmp3, AcceptedCmp4, AcceptedCmp5, Response*. Total number of positive responses to all campaigns.







"""

data["Total_Num"] = data["NumWebPurchases"] + data["NumCatalogPurchases"] + data["NumStorePurchases"]

data["Total_Mnt"] = data["MntWines"] + data["MntFruits"] + data["MntMeatProducts"] + data["MntFishProducts"] + data["MntSweetProducts"] + data["MntGoldProds"]

data["Age"] = 2024 - data["Year_Birth"]

data['Living_Situation'] = data['Marital_Status'].apply(lambda x: 'With Partner' if x in ['Married', 'Together'] else 'Alone')

data["Num_Children"] = data["Kidhome"] + data["Teenhome"]

data["Is_Parent"] = (data['Num_Children'] > 0).astype(int)

data["Family_Size"] = data['Living_Situation'].apply(lambda x: 2 if x == 'With Partner' else 1) + data['Num_Children']

data["Education"]= data["Education"].replace({"Basic":"Undergraduate_n_Below","2n Cycle":"Undergraduate_n_Below", "Graduation":"Graduate_n_Above", "Master":"Graduate_n_Above", "PhD":"Graduate_n_Above"})

data["Campaigns_Accepted"] = data["AcceptedCmp1"] + data["AcceptedCmp2"] + data["AcceptedCmp3"] + data["AcceptedCmp4"] + data["AcceptedCmp5"] + data["Response"]

# drop features we no longer need or would not impact our analysis
columns_to_drop = ["Marital_Status", "Year_Birth", "Kidhome", "Teenhome", "AcceptedCmp1", "AcceptedCmp2", "AcceptedCmp3", "AcceptedCmp4", "AcceptedCmp5", "Response", "ID", "Z_CostContact", "Z_Revenue"]
data = data.drop(columns_to_drop, axis=1)
data.info()

data.describe()

"""Next, we want to check if there are any outliers using box plots for *Income*, *Total_Mnt*, and *Age*."""

features_for_box_plots = ['Income', 'Total_Mnt', 'Age']
plt.figure(figsize=(12, 6))
for i, feature in enumerate(features_for_box_plots, 1):
    plt.subplot(1, len(features_for_box_plots), i)
    sns.boxplot(y=data[feature])
    plt.title(f'Box plot - {feature}')
plt.tight_layout()
plt.show()

"""We can see from the box plots above, there are some outliers in both *Income* and *Total_Mnt*. Although no clear outliers are visible in the box plot for *Age*, we can see from the stats that the max *Age* in the dataset is 131 because this dataset was collected a long time ago. We are going to set the upper limit of *Age* to 100 and also remove the other outliers."""

selected_columns = ['Income', 'Total_Mnt', 'Age']
Q1 = data[selected_columns].quantile(0.25)
Q3 = data[selected_columns].quantile(0.75)
IQR = Q3 - Q1

# outlier thresholds
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

# filter outliers and reset index
data = data[(data["Age"]< 100)]
data = data[(data['Income'] <= upper_bound['Income']) & (data['Income'] >= lower_bound['Income'])]
data = data[(data['Total_Mnt'] <= upper_bound['Total_Mnt']) & (data['Total_Mnt'] >= lower_bound['Total_Mnt'])]
data.reset_index(drop=True, inplace=True)
data.describe()

"""To visually observe and present correlations among features in our dataset, we are going to use a correlation matrix along with a heatmap."""

numeric_data = data.select_dtypes(include=[np.number])
correlation_matrix = numeric_data.corr()

plt.figure(figsize=(20, 20))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.show()

"""From the heat map we can see the following patterns:
- There are strong positive correlations among different spending categories implying that customers who spend more in one category are also likely to spend more in others.
- *Total_Num* is positively correlated with specific channels of purchases showing that customers who purchase more often in one channel tend to have more active purchasing activities overall.
- *NumWebVisitsMonth* is negatively correlated with *NumWebPurchases*, implying that higher website visits do not necessarily result in more purchases.
- *NumDealsPurchases* shows a relatively strong positive correlation with *NumWebPurchases*, suggesting that promotions or discounts might be effective in encouraging online purchases.

## Data preprocessing

Apply standard scaler to ensure each feature's equal contribution and avoiding bias.
"""

scaler = StandardScaler()
scaled_data = scaler.fit_transform(numeric_data)
scaled_data_df = pd.DataFrame(scaled_data, columns=numeric_data.columns)

print("Scaled data frame:")
scaled_data_df.head()

"""## Dimensionality reduction (PCA)

Now that our data is properly scaled, we can proceed with Principal Component Analysis (PCA).
"""

pca = PCA(n_components=3)
data_pca = pca.fit_transform(scaled_data_df)

fig = plt.figure(figsize=(10, 7))
ax = fig.add_subplot(111, projection='3d')

scatter = ax.scatter(data_pca[:, 0], data_pca[:, 1], data_pca[:, 2],
                     c=data_pca[:, 2], cmap='viridis', marker='o')

colorbar = fig.colorbar(scatter, ax=ax, extend='both')

ax.set_xlabel('PC 1')
ax.set_ylabel('PC 2')
ax.set_zlabel('PC 3')
ax.set_title('3D PCA scatter plot')

plt.show()

"""


## Hierarchical clustering"""

Z = linkage(data_pca, method='ward')

plt.figure(figsize=(12, 8))
dendrogram(Z,
           truncate_mode='lastp',
           p=12,
           leaf_rotation=45.,
           leaf_font_size=15.,
           show_contracted=True)
plt.title('Hierarchical clustering dendrogram')
plt.xlabel('Cluster size')
plt.ylabel('Distance')
plt.axhline(y=20, color='r', linestyle='--')
plt.show()

"""The dendrogram shows clusters merging at various levels of distance. Some clusters combine at a very high distance, suggesting that these clusters are quite dissimilar compared to others that merge at lower distances.

The distribution of cluster sizes shows a mix of smaller and larger clusters. Some clusters having a substantial number of points (e.g., cluster sizes of 559) and others being very small (sizes like 60). This wide variation in cluster sizes might reflect inherent groupings in the data but could also pose challenges in terms of cluster interpretability.

Due to the varied distances and the uneven cluster sizes, hierarchical clustering might not be the most practical choice for creating customer segments in this case. We will conduct a detailed evaluation to assess its performance.


"""

max_d = 20  # distance cutoff
data['Cluster_H'] = fcluster(Z, max_d, criterion='distance')

plt.figure(figsize=(12, 6))
for i in set(data['Cluster_H']):
    plt.scatter(data.loc[data['Cluster_H'] == i, 'Total_Mnt'], data.loc[data['Cluster_H'] == i, 'Age'], label=f'Cluster {i}')
plt.title('Hierarchical Clusters visualization on Total_Mnt and Age')
plt.xlabel('Total_Mnt')
plt.ylabel('Age')
plt.legend()
plt.show()

"""**Observations**:

The visualization of the hierarchical clusters shows significant overlapping, indicating that the clusters are not well-separated.

Based on both the dendrogram and the visualization, it appears that hierarchical clustering may not be the best choice for our needs.

We have decided to explore other clustering methods, such as K-means or density-based methods like DBSCAN, which can handle outliers more effectively. These methods might provide more balanced and applicable clusters.

## DBSCAN (Density-Based Spatial Clustering of Applications with Noise)
"""

# DBSCAN clustering
dbscan = DBSCAN(eps=0.5, min_samples=5)
clusters = dbscan.fit_predict(scaled_data_df)
data['Cluster_DBSCAN'] = clusters

from mpl_toolkits.mplot3d import Axes3D

# count cluster labels to see how many clusters were formed & how many outliers were detected
cluster_counts = pd.Series(clusters).value_counts()
print("Cluster counts:\n", cluster_counts)

fig = plt.figure(figsize=(10, 8))
ax = fig.add_subplot(111, projection='3d')

# scatter plot using the first three principal components
scatter = ax.scatter(data_pca[:, 0], data_pca[:, 1], data_pca[:, 2],
                     c=clusters,
                     cmap='viridis',
                     marker='o',
                     alpha=0.6)


colorbar = plt.colorbar(scatter, ax=ax)
colorbar.set_label('Cluster Labels')

ax.set_xlabel('Principal Component 1')
ax.set_ylabel('Principal Component 2')
ax.set_zlabel('Principal Component 3')
ax.set_title('DBSCAN Clustering in 3D')

plt.show()

"""The chart suggests that DBSCAN has identified most of the data as belonging to a single cluster. This indicates that the chosen eps value might not be suitable, leading DBSCAN to consider most points as part of the same cluster.

Next, we will draw a K-Nearest Neighbors Distances plot to find the optimal eps.
"""

# use NearestNeighbors to find optimal eps
nn = NearestNeighbors(n_neighbors=2)
nbrs = nn.fit(scaled_data_df)
distances, indices = nbrs.kneighbors(scaled_data_df)

distances = np.sort(distances, axis=0)
distances = distances[:, 1]
plt.figure(figsize=(10, 6))
plt.plot(distances)
plt.title('K-Nearest Neighbors Distances')
plt.xlabel('Points sorted by distance')
plt.ylabel('Epsilon distance')
plt.show()

"""According to the polt, we can observe a sudden inscrease shows around 4, so the optimal eps should be 4."""

# update eps
dbscan = DBSCAN(eps=4, min_samples=5)
clusters = dbscan.fit_predict(scaled_data_df)

# count cluster labels to see how many clusters were formed & how many outliers were detected
cluster_counts = pd.Series(clusters).value_counts()
print("Cluster counts:\n", cluster_counts)

fig = plt.figure(figsize=(10, 8))
ax = fig.add_subplot(111, projection='3d')

# scatter plot using the first three principal components
scatter = ax.scatter(data_pca[:, 0], data_pca[:, 1], data_pca[:, 2],
                     c=clusters,
                     cmap='viridis',
                     marker='o',
                     alpha=0.6)


colorbar = plt.colorbar(scatter, ax=ax)
colorbar.set_label('Cluster Labels')

ax.set_xlabel('Principal Component 1')
ax.set_ylabel('Principal Component 2')
ax.set_zlabel('Principal Component 3')
ax.set_title('DBSCAN Clustering in 3D')

plt.show()

"""According to the plot, the majority of data points are still identified as part of one large cluster. This could indicate that the data does not contain distinct, dense regions required by DBSCAN. It's better to consider other clustering methods.

## K-means clustering
"""

k_values = range(1, 11)
inertias = []

for k in k_values:
    model = KMeans(n_clusters=k, random_state=42)
    model.fit(data_pca)
    inertias.append(model.inertia_)

plt.figure(figsize=(10, 6))
plt.plot(k_values, inertias, '-o')
plt.title('Elbow method to find optimal k')
plt.xlabel('Num clusters (k)')
plt.ylabel('Inertia')
plt.xticks(k_values)
plt.grid(True)
plt.show()

"""Based on the elbow method graph, we pick k=3 for clustering because it's where the decrease in inertia starts to level off, suggesting that three clusters are enough to capture the main structure of the data without adding unnecessary complexity."""

# choose the optimal k from the elbow curve
k_optimal = 3

# apply K-Means to the dataset
kmeans = KMeans(n_clusters=k_optimal, init='k-means++', max_iter=300, n_init=10, random_state=0)
cluster_labels = kmeans.fit_predict(data_pca)

data_pca_df = pd.DataFrame(data_pca, columns=['PC1', 'PC2', 'PC3'])
data_pca_df['Cluster'] = cluster_labels

fig = plt.figure(figsize=(11, 8))
ax = fig.add_subplot(111, projection='3d')
scatter = ax.scatter(data_pca_df['PC1'], data_pca_df['PC2'], data_pca_df['PC3'], c=data_pca_df['Cluster'], cmap='viridis')
ax.set_xlabel('PC 1')
ax.set_ylabel('PC 2')
ax.set_zlabel('PC 3')
ax.set_title('K-means clustering graph')
plt.show()

"""### Evaluating the Graph:

In the graph, each cluster shows different levels of density which is common in complex data. The teal cluster stands out because it seems to include a special kind of customers with unique traits or behaviors that set them apart from others.

The purple and yellow clusters overlap a bit but we can still tell them apart. This overlap suggests that these groups share some characteristics but also have their own unique features, as shown by the analysis methods we used.

Overall, the groups are mostly well-separated, which is great for our purpose of customer segmentation. The small overlaps are normal and show that there are subtle connections between the customer groups, which we'll explore further.

Next, we'll use numerical analysis to measure how far apart these groups are. This step will confirm what we see in the graph and help us come up with marketing strategies based on these segments.


"""

# add original features back to the PCA DataFrame for interpretation
data_pca_df = pd.concat([data_pca_df, data.reset_index(drop=True)], axis=1)

# calculate average values for important features
cluster_summary = data_pca_df.groupby('Cluster').agg({
    'Income': 'mean',
    'Total_Mnt': 'mean',
    'Age': 'mean',
    'Total_Num': 'mean',
    'Num_Children': 'mean',
    'Family_Size': 'mean',
    'Campaigns_Accepted': 'mean'
}).reset_index()

print("Cluster Summary:")
display(cluster_summary)

# visualize the distribution of key features
features_to_visualize = ['Income', 'Total_Mnt', 'Age', 'Total_Num', 'Num_Children', 'Family_Size','Campaigns_Accepted']
fig, axes = plt.subplots(1, len(features_to_visualize), figsize=(20, 5))
for i, feature in enumerate(features_to_visualize):
    sns.boxplot(ax=axes[i], x='Cluster', y=feature, data=data_pca_df)
    axes[i].set_title(f'Distribution of {feature} by Cluster')
plt.tight_layout()
plt.show()

# examine specific behaviors
print("Detailed examination by cluster:")
for i in range(k_optimal):
    print(f"\nCluster {i}:")
    display(data_pca_df[data_pca_df['Cluster'] == i].describe())

"""## Evaluating models

### Metrics

#### Silhouette Score

The Silhouette Score ranges from -1 to 1. A high score near +1 indicates that the clusters are well apart from each other and clearly defined.

#### Calinski-Harabasz Score

This score is higher when clusters are dense and well-separated, which relates to a model with better-defined clusters.

#### Davies-Bouldin Score

The Davies-Bouldin Score reflects the average 'similarity' between clusters. Similarity is a measure that compares the distance between clusters with the size of the clusters themselves. Lower values indicate better clustering.
"""

data_pca_df = pd.DataFrame(data_pca, columns=['PC1', 'PC2', 'PC3'])
kmeans = KMeans(n_clusters=3, init='k-means++', max_iter=300, n_init=10, random_state=0)
cluster_labels = kmeans.fit_predict(data_pca_df)

silhouette = silhouette_score(data_pca_df, cluster_labels)
calinski_harabasz = calinski_harabasz_score(data_pca_df, cluster_labels)
davies_bouldin = davies_bouldin_score(data_pca_df, cluster_labels)

print(f"Silhouette Score: {silhouette}")
print(f"Calinski-Harabasz Index: {calinski_harabasz}")
print(f"Davies-Bouldin Index: {davies_bouldin}")

"""### Results

#### Silhouette Score: 0.443

A score of 0.443 is moderate, suggesting that the clusters are reasonably well-separated but not distinct. This score indicates a moderate separation among our customer clusters. While the customer groups are reasonably distinguishable, the score suggests some overlap and this is expected in a diverse customer base. This indicates we are effectively identifying distinct customer behaviors but also highlights the complexity of customer preferences.

#### Calinski-Harabasz Index: 2545.553

The high value of this index confirms that our customer segmentation is good, showing that each segment is internally cohesive while being distinct from others. This strong separation supports our strategy to tailor marketing efforts according to the specific characteristics and needs of each segment.

#### Davies-Bouldin Index: 0.891

Although lower values would be ideal, this score still reflects fairly well-separated clusters. It confirms that our segments are distinct enough to warrant customized marketing approaches for each, enabling more targeted and effective marketing campaigns.

#### Overall Assessment:

The clustering analysis, both visual and numerical, confirms that the K-means algorithm has performed effectively. The clusters identified through the algorithm are generally distinct and separate well. The K-means algorithm has effectively grouped customers into distinct categories.

## Profiling

The box plots reveal distinct patterns among the three clusters, helping us understand the unique characteristics of each customer group.

### Cluster 0: Active shoppers

High-income earners with significant spending on products, fewer children, and typically smaller family sizes. They engage in a high number of transactions, suggesting active purchasing behavior. This cluster is also the most responsive to marketing efforts.

Suggested Strategy:

1. Focus on high-end products
2. Implement loyalty programs with exclusive benefits
3. Create personalized marketing campaigns that highlight premium service and product quality

### Cluster 1: Cost-conscious buyers

Lower-income customers characterized by the lowest levels of spending and transaction numbers, possibly younger or facing more constraints on discretionary spending.

Suggested Strategy:

1. Target with discounts and bulk purchase deals
2. Offer financial services or payment plans to encourage purchasing

### Cluster 2: Family shoppers

Middle-income earners with moderate spending and transaction activity, but slightly larger family sizes and a moderate response to marketing campaigns.

Suggested Strategy:

1. Promote products that appeal to larger families.
2. Focus on promotions for kid- and teen-friendly products.
3. Organize campaigns centered around holidays or family events.

### Conclusion
By tailoring our marketing strategies to the specific needs and characteristics of each cluster, we can improve customer engagement and drive sales more effectively. Each cluster's distinct profile suggests targeted approaches that align with their unique preferences and financial capacities.
"""